{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization\n",
    "This part includes all the imports as well as the installation of the modules needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install tqdm confluent-kafka matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of utility code to keep the credentials out of the github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.cache\n",
    "def read_ccloud_config(config_file='client.properties'):\n",
    "    conf = {}\n",
    "    with open(config_file) as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if len(line) != 0 and line[0] != \"#\":\n",
    "                parameter, value = line.strip().split('=', 1)\n",
    "                conf[parameter] = value.strip()\n",
    "    return conf\n",
    "def read_ccloud_producer_config(config_file='client.properties'):\n",
    "    conf = read_ccloud_config(config_file)\n",
    "    omitted_fields = set(['schema.registry.url', 'basic.auth.credentials.source', 'basic.auth.user.info'])\n",
    "    omitted_prefix = 'confluent'\n",
    "    for fld in list(conf.keys()):\n",
    "        if fld in omitted_fields or fld.startswith(omitted_prefix):\n",
    "            conf.pop(fld, None)\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic Kafka Producer with a simple approach to data rate, payload size, and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "from datetime import datetime, timezone\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "def delivery_callback(err, msg):\n",
    "    if err:\n",
    "        print('ERROR: Message failed delivery: {}'.format(err))\n",
    "    else:\n",
    "        #print(\"Produced event to topic {topic}: key = {key:12} value = {value:12}\".format(\n",
    "        #    topic=msg.topic(), key=msg.key().decode('utf-8'), value=msg.value().decode('utf-8')))\n",
    "        pass\n",
    "\n",
    "def getMessages(numMessages, msgSize):\n",
    "    # len is 64\n",
    "    base_msg = \"Upon our honor, we will monitor our data streaming application. \"\n",
    "    for i in range(numMessages):\n",
    "        yield { 'key': f\"mt_key_{randint(1,6)}\", 'value': f\"{base_msg * (msgSize//len(base_msg))}\" }\n",
    "\n",
    "def publishMessages(load_params):\n",
    "    startTime = datetime.now(timezone.utc)\n",
    "\n",
    "    # Simulating extra connections\n",
    "    producers = []\n",
    "    for i in range(load_params['num_producers']):\n",
    "        producers.append(Producer(read_ccloud_producer_config()))\n",
    "    \n",
    "    msgSentCount = 0\n",
    "    numMessages = load_params['msg_rate_per_s'] * load_params['msg_send_duration_s']\n",
    "    msgSize = load_params['msg_size_bytes']\n",
    "    progressBar = tqdm(getMessages(numMessages, msgSize), total=numMessages, unit=' messages')\n",
    "    for msg in progressBar:\n",
    "        ts = datetime.now(timezone.utc)\n",
    "        ts_str = ts.isoformat()\n",
    "        msg['value'] = '{ \"payload\": \"' + msg['value'] + '\", \"ts\": \"' + ts_str + '\" }'\n",
    "        producers[msgSentCount % load_params['num_producers']].produce(\"sale_records\", key=msg['key'], value=msg['value'], callback=delivery_callback)\n",
    "        msgSentCount += 1\n",
    "\n",
    "        # Rate Limiter\n",
    "        if not msgSentCount % 100:\n",
    "            elapsed_seconds = (ts - startTime).total_seconds()\n",
    "            if msgSentCount >=  elapsed_seconds * load_params['msg_rate_per_s']:\n",
    "                sleep(msgSentCount/load_params['msg_rate_per_s'] - elapsed_seconds)            \n",
    "\n",
    "    for producer in producers:\n",
    "        producer.flush()\n",
    "    \n",
    "    endTime = datetime.now(timezone.utc)\n",
    "\n",
    "    return startTime, endTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calls to the Confluent Metrics API to get the metrics we care about. A small bit of poetic license here where cluster_load is only available for Dedicated clusters on the Confluent Cloud. So it is computed here instead; taking into account the various dependent resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "MetricsQueries = {\n",
    "    'received_bytes': {\n",
    "        'title': 'Ingress',\n",
    "        'yaxis_title': 'MB',\n",
    "        'yaxis_limit': 150,\n",
    "        'scale_factor': 1024*1024,\n",
    "        'query': {\n",
    "            'aggregations': [{ 'metric': 'io.confluent.kafka.server/received_bytes'}],\n",
    "            'group_by': ['metric.topic']\n",
    "        }\n",
    "    },\n",
    "    'sent_bytes': {\n",
    "        'title': 'Egress',\n",
    "        'yaxis_title': 'MB',\n",
    "        'yaxis_limit': 150,\n",
    "        'scale_factor': 1024*1024,\n",
    "        'query': {\n",
    "            'aggregations': [{ 'metric': 'io.confluent.kafka.server/sent_bytes'}],\n",
    "            'group_by': ['metric.topic']\n",
    "        }\n",
    "    },\n",
    "    'active_connection_count' : {\n",
    "        'title': 'Active Connection Count',\n",
    "        'yaxis_title': 'Count',\n",
    "        'yaxis_limit': 100,\n",
    "        'query': {\n",
    "            \"aggregations\":[{ \"metric\":\"io.confluent.kafka.server/active_connection_count\"}]\n",
    "        }\n",
    "    },\n",
    "    'request_count': {\n",
    "        'title': 'Request Count',\n",
    "        'yaxis_title': 'Count',\n",
    "        'query': {\n",
    "            'aggregations': [{ 'metric': 'io.confluent.kafka.server/request_count'}]\n",
    "        }\n",
    "    },\n",
    "    'cluster_load': {\n",
    "        'title': 'Cluster Load',\n",
    "        'yaxis_title': '% Load',\n",
    "        'yaxis_limit': 100\n",
    "    }\n",
    "}\n",
    "def getMetrics(startTime, endTime):\n",
    "    conf = read_ccloud_config()\n",
    "    url = conf['confluent.metrics.endpoint']\n",
    "    headers = {\n",
    "        'Authorization': f\"Basic {conf['confluent.cloud_api_token']}\",\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    common = {\n",
    "        \"filter\":{\"op\":\"OR\",\"filters\":[{\"field\":\"resource.kafka.id\",\"op\":\"EQ\",\"value\":\"lkc-v1jq15\"}]},\n",
    "        \"granularity\":\"PT1M\",\n",
    "        \"limit\":1000\n",
    "    }\n",
    "    interval = {\n",
    "        \"intervals\":[f\"{startTime.isoformat(timespec='seconds')}/{endTime.isoformat(timespec='seconds')}\"],\n",
    "    }\n",
    "\n",
    "    responses = {}\n",
    "    for qry in MetricsQueries:\n",
    "        if 'query' not in MetricsQueries[qry]:\n",
    "            continue\n",
    "        data = MetricsQueries[qry]['query'] | common | interval\n",
    "\n",
    "        req = urllib.request.Request(url, json.dumps(data).encode('utf-8'), headers)\n",
    "        resp = urllib.request.urlopen(req)\n",
    "        if resp.getcode() == 200:\n",
    "            responses[qry] = json.loads(resp.read())\n",
    "            if 'scale_factor' in MetricsQueries[qry]:\n",
    "                responses[qry]['data'] = list(map(\n",
    "                    lambda x: { \n",
    "                        'timestamp': x['timestamp'], \n",
    "                        'value': x['value']/MetricsQueries[qry]['scale_factor'] \n",
    "                    },\n",
    "                    responses[qry]['data']))\n",
    "        else:\n",
    "            print(\"Error: {resp.getcode()}\")\n",
    "\n",
    "    return responses\n",
    "\n",
    "def getAllMetrics(startTime, endTime):\n",
    "    # give the metrics a chance to get done\n",
    "    # for the ever eager run all in notebook fans\n",
    "    timeSinceProduce = (datetime.now(timezone.utc) - endTime).total_seconds()\n",
    "    if  timeSinceProduce < 60:\n",
    "        sleep(60-timeSinceProduce)\n",
    "        \n",
    "    results = getMetrics(startTime, endTime)\n",
    "    cluster_load_data = []\n",
    "    # pick any metric to get number of entries since we aggregate per minute\n",
    "    for i in range(len(results['active_connection_count']['data'])):\n",
    "        cl_pct = 0\n",
    "        cl_ts = \"\"\n",
    "        for metric in MetricsQueries:\n",
    "            if 'query' not in MetricsQueries[metric] or i >= len(results[metric]['data']):\n",
    "                continue\n",
    "            cl_pct = max(cl_pct, 100.0 * results[metric]['data'][i]['value'] / confluent_max[metric])\n",
    "            cl_ts = max(cl_ts, results[metric]['data'][i]['timestamp'])\n",
    "        cluster_load_data.append({ 'timestamp': cl_ts, 'value': cl_pct })\n",
    "    results['cluster_load'] = { 'data': cluster_load_data }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using matplotlib to plot our graphs in a semi-sane way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import axes\n",
    "%matplotlib inline\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "def addPlot(axs, row, col, data, title, yAxisTitle, yAxisLimit, startTime):\n",
    "    x_values = [(datetime.fromisoformat(el['timestamp']) - startTime).total_seconds() for el in data]\n",
    "    y_values = [el['value'] for el in data]\n",
    "\n",
    "    if isinstance(axs[0], axes.Axes):\n",
    "        axis = axs[col]\n",
    "    else:\n",
    "        axis = axs[row,col]\n",
    "\n",
    "    axis.plot(x_values, y_values, marker='s')\n",
    "    # Add labels and title\n",
    "    axis.set_xlabel('Time (s)')\n",
    "\n",
    "    axis.set_ylabel(yAxisTitle)\n",
    "    if yAxisLimit:\n",
    "        axis.set_ybound(lower=0, upper=yAxisLimit)\n",
    "    else:\n",
    "        axis.set_ybound(lower=0)\n",
    "        \n",
    "    axis.set_title(title)\n",
    "\n",
    "\n",
    "def plotGraphs(results, graphs, startTime):\n",
    "    index = 0\n",
    "    total_plots = len(graphs)\n",
    "    ncols = 2\n",
    "    nrows = ceil(total_plots/2)\n",
    "    fig, axs = plt.subplots(nrows,ncols,sharex=True, figsize=(8, 3*nrows))\n",
    "    for qry in graphs:\n",
    "        addPlot(axs, index//ncols, index%ncols, results[qry]['data'],  \n",
    "                title=MetricsQueries[qry]['title'], \n",
    "                yAxisTitle=MetricsQueries[qry]['yaxis_title'],\n",
    "                yAxisLimit=MetricsQueries[qry].get('yaxis_limit'),\n",
    "                startTime=startTime)\n",
    "        index += 1\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions for the cluster Limits from https://docs.confluent.io/cloud/current/clusters/cluster-types.html#types-basic-limits-per-cluster on 12/1/2023. Things to note\n",
    "- The extra *60 is because the metrics are aggregated by the minute\n",
    "- one of these has had a bit of artistic license applied to simulate an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confluent_max = {\n",
    "    'active_connection_count': 100,\n",
    "    'received_bytes': 250 * 1024 * 1024 * 60,\n",
    "    'sent_bytes': 750 * 1024 * 1024 * 60,\n",
    "    'request_count': 15000 * 60\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The original problem reproduced with a 4096 byte message size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Generation\n",
    "load_params = {\n",
    "    'msg_size_bytes': 4096,\n",
    "    'msg_rate_per_s': 500,\n",
    "    'msg_send_duration_s': 180,\n",
    "    'num_producers': 20 \n",
    "}\n",
    "\n",
    "startTime, endTime = publishMessages(load_params)\n",
    "results = getAllMetrics(startTime, endTime)\n",
    "plotGraphs(results, ['cluster_load', 'received_bytes'], startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now with half the payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Generation\n",
    "load_params = {\n",
    "    'msg_size_bytes': 2048,\n",
    "    'msg_rate_per_s': 500,\n",
    "    'msg_send_duration_s': 180,\n",
    "    'num_producers': 20 \n",
    "}\n",
    "\n",
    "startTime, endTime = publishMessages(load_params)\n",
    "results = getAllMetrics(startTime, endTime)\n",
    "plotGraphs(results, ['cluster_load', 'received_bytes'], startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The final analysis showing what the real factor is causing the high cluster load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGraphs(results, ['cluster_load', 'received_bytes', 'active_connection_count', 'request_count'], startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Solution\n",
    "What does reducing the number of publishers do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Generation\n",
    "load_params = {\n",
    "    'msg_size_bytes': 4096,\n",
    "    'msg_rate_per_s': 500,\n",
    "    'msg_send_duration_s': 180,\n",
    "    'num_producers': 10 \n",
    "}\n",
    "\n",
    "startTime, endTime = publishMessages(load_params)\n",
    "results = getAllMetrics(startTime, endTime)\n",
    "plotGraphs(results, ['cluster_load', 'received_bytes', 'active_connection_count', 'request_count'], startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpers for the Puzzle\n",
    "A few code snippets to help in the investigations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use the AdminClient to query the cluster for the current list of brokers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.admin import AdminClient\n",
    "\n",
    "adminClient = AdminClient(read_ccloud_producer_config()) \n",
    "clusterMetadata = adminClient.describe_cluster().result()\n",
    "\n",
    "for node in clusterMetadata.nodes:\n",
    "    print(f\"id: {node.id}, id_string: {node.id_string}, host: {node.host}, port: {node.port}, rack: {node.rack}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to find the current leader broker and ISRs for each partition in the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicsMetadata = adminClient.list_topics()\n",
    "\n",
    "for topic, topicMetadata in topicsMetadata.topics.items():\n",
    "    for pid, partition in topicMetadata.partitions.items():\n",
    "        print(f\"Partition id {partition.id}, leader: {partition.leader}, replicas: {partition.replicas}, isrs: {partition.isrs}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get statistics out of our Producer every second. Add these into the producer code to generate stats every second. \n",
    "\n",
    "This code doesn't do anything by itself. It needs to be added to the producer code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_cb(s):\n",
    "    j = json.loads(s)\n",
    "    for m in ['tx_bytes', 'txmsgs']:\n",
    "        print(f\"{m}: {j[m]} \")\n",
    "        \n",
    "\n",
    "conf = read_ccloud_producer_config()\n",
    "conf['stats_cb'] = stats_cb\n",
    "conf['statistics.interval.ms'] = 1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
