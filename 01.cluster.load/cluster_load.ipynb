{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunks of code we will need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://aws:****@confluent-519856050701.d.codeartifact.us-west-2.amazonaws.com/pypi/pypi/simple/\n",
      "Requirement already satisfied: confluent-kafka in /Users/amazhar/.pyenv/versions/3.12.0/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: tabulate in /Users/amazhar/.pyenv/versions/3.12.0/lib/python3.12/site-packages (0.9.0)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install confluent-kafka tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of utility code to keep the credentials out of the github repo. There is an easy startup guide for using Confluent Cloud over at https://developer.confluent.io/get-started/python/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# Read the config file; cache it\n",
    "@functools.cache\n",
    "def read_ccloud_config(config_file='client.properties'):\n",
    "    conf = {}\n",
    "    with open(config_file) as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if len(line) != 0 and line[0] != \"#\":\n",
    "                parameter, value = line.strip().split('=', 1)\n",
    "                conf[parameter] = value.strip()\n",
    "    return conf\n",
    "\n",
    "# Skip non-Kafka client properties\n",
    "def read_ccloud_producer_config(config_file='client.properties'):\n",
    "    conf = read_ccloud_config(config_file)\n",
    "    omitted_fields = set(['schema.registry.url', 'basic.auth.credentials.source', 'basic.auth.user.info'])\n",
    "    omitted_prefix = 'confluent'\n",
    "    for fld in list(conf.keys()):\n",
    "        if fld in omitted_fields or fld.startswith(omitted_prefix):\n",
    "            conf.pop(fld, None)\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic Kafka Producer with a simple approach to data rate, payload size, and keys\n",
    "Some documentation:\n",
    "- The python kafka client library is described at https://docs.confluent.io/kafka-clients/python/current/overview.html\n",
    "- The metrics delivered via the callback are documented at librdkafka https://github.com/confluentinc/librdkafka/blob/master/STATISTICS.md\n",
    "- Configuration parameters for the Publisher are also as librdkafka https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md\n",
    "- Callbacks are at the python client documentation https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html#kafka-client-configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timezone\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import json\n",
    "\n",
    "\n",
    "def get_stats_cb(results):\n",
    "    # Accumulating, sort of, the statistics so we can aggregate at the end\n",
    "    def stats_cb(s):\n",
    "        j = json.loads(s)\n",
    "        # Overwriting each time since we just need the last one\n",
    "        results[j['name']].append(j)\n",
    "    return stats_cb\n",
    "        \n",
    "def get_delivery_callback(latencies):\n",
    "    def delivery_callback(err, msg):\n",
    "        if err:\n",
    "            print('ERROR: Message failed delivery: {}'.format(err))\n",
    "        else:\n",
    "            latencies.append(msg.latency())\n",
    "    return delivery_callback\n",
    "\n",
    "def getMessages(numMessages, msgSize):\n",
    "    num_partitions = 6 # Our topic is configured as such\n",
    "    # len is 64 for the below string\n",
    "    base_msg = \"Upon our honor, we will monitor our data streaming application. \"\n",
    "    for i in range(numMessages):\n",
    "        yield { 'key': f\"mt_key_{i % num_partitions}\", 'value': f\"{base_msg * (msgSize//len(base_msg))}\" }\n",
    "\n",
    "def publishMessages(load_params):\n",
    "    startTime = datetime.now(timezone.utc)\n",
    "\n",
    "    # Simulating extra connections\n",
    "    conf = read_ccloud_producer_config()\n",
    "    statistics_interval_ms = 250\n",
    "    stats = defaultdict(list)\n",
    "    conf['stats_cb'] = get_stats_cb(stats)\n",
    "    conf['statistics.interval.ms'] = statistics_interval_ms\n",
    "    if 'extra_producer_args' in load_params.keys():\n",
    "        conf.update(load_params['extra_producer_args'])\n",
    "\n",
    "    producers = [ Producer(conf) for i in range(load_params['num_producers']) ]\n",
    "    \n",
    "    msgSentCount = 0\n",
    "    numMessages = load_params['num_msgs']\n",
    "    msgSize = load_params['msg_size_bytes']\n",
    "    msgRateSleepTimeSecs = 1 / load_params['msg_rate_per_s'] \n",
    "\n",
    "    latencies = []\n",
    "    delivery_callback = get_delivery_callback(latencies)\n",
    "\n",
    "    for msg in getMessages(numMessages, msgSize):\n",
    "        ts = datetime.now(timezone.utc)\n",
    "        ts_str = ts.isoformat()\n",
    "        msg['value'] = '{ \"payload\": \"' + msg['value'] + '\", \"ts\": \"' + ts_str + '\" }'\n",
    "        producer_index = msgSentCount % load_params['num_producers']\n",
    "        producers[producer_index].produce(\"sale_records\", key=msg['key'], value=msg['value'],\n",
    "                                                       callback=delivery_callback)\n",
    "        if not msgSentCount % 100:\n",
    "            for producer in producers:\n",
    "                producer.poll()\n",
    "        msgSentCount += 1\n",
    "        sleep(msgRateSleepTimeSecs)           \n",
    "\n",
    "    produceEndTime = datetime.now(timezone.utc)\n",
    "\n",
    "    for producer in producers:\n",
    "        producer.flush()\n",
    "    \n",
    "    endTime = datetime.now(timezone.utc)\n",
    "\n",
    "    return startTime, produceEndTime, endTime, latencies, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calls to the Confluent Metrics API to get the metrics we care about. \n",
    "- Documentation for it is at https://docs.confluent.io/cloud/current/monitoring/metrics-api.html. \n",
    "- A complete list of available cluster metrics is at https://api.telemetry.confluent.cloud/docs/descriptors/datasets/cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "MetricsQueries = {\n",
    "    'received_bytes': {\n",
    "        'query': {\n",
    "            'aggregations': [{ 'metric': 'io.confluent.kafka.server/received_bytes'}],\n",
    "            'group_by': ['metric.topic']\n",
    "        }\n",
    "    },\n",
    "    'active_connection_count' : {\n",
    "        'query': { \"aggregations\":[{ \"metric\":\"io.confluent.kafka.server/active_connection_count\"}] }\n",
    "    },\n",
    "    'request_count': {\n",
    "        'query': { 'aggregations': [{ 'metric': 'io.confluent.kafka.server/request_count'}] }\n",
    "    },\n",
    "    'received_records': {\n",
    "        'query': { 'aggregations': [{ 'metric': 'io.confluent.kafka.server/received_records'}]}\n",
    "    }\n",
    "}\n",
    "def getMetrics(startTime, endTime):\n",
    "    # The Metrics API aggregates by the minute, and throw in clock skew\n",
    "    sleep(60)\n",
    "    startTime -= timedelta(seconds=60)\n",
    "    endTime += timedelta(seconds=60)\n",
    "\n",
    "    conf = read_ccloud_config()\n",
    "    url = conf['confluent.metrics.endpoint']\n",
    "    headers = {\n",
    "        'Authorization': f\"Basic {conf['confluent.cloud_api_token']}\",\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    common = {\n",
    "        \"filter\":{\"op\":\"OR\",\"filters\":[{\"field\":\"resource.kafka.id\",\"op\":\"EQ\",\"value\":\"lkc-v1jq15\"}]},\n",
    "        \"granularity\":\"PT1M\",\n",
    "        \"limit\":1000\n",
    "    }\n",
    "    interval = {\n",
    "        \"intervals\":[f\"{startTime.isoformat(timespec='seconds')}/{endTime.isoformat(timespec='seconds')}\"],\n",
    "    }\n",
    "\n",
    "    responses = {}\n",
    "    for qry in MetricsQueries:\n",
    "        data = MetricsQueries[qry]['query'] | common | interval\n",
    "\n",
    "        req = urllib.request.Request(url, json.dumps(data).encode('utf-8'), headers)\n",
    "        resp = urllib.request.urlopen(req)\n",
    "        if resp.getcode() == 200:\n",
    "            responses[qry] = json.loads(resp.read())\n",
    "        else:\n",
    "            print(f\"Error: {resp.getcode()}, Request was {json.dumps(data)}\")\n",
    "\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main code to execute a sample load. Given parameters of the number of producers, the message size, and the number of messages, it sends out all the messages, waits for acknowledgements, collects client side metrics, and calls the metrics api to get back cluster metrics. It also includes a simple tabular print utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "def execute_simulation_app(load_params):\n",
    "\n",
    "    start, endProduce, end, latencies,stats = publishMessages(load_params)\n",
    "    print(f\"Done in {(end-start).seconds} s Start:{start}, End:{end})\")\n",
    "\n",
    "    result = [\n",
    "        load_params['num_producers'],\n",
    "        load_params['num_msgs'],\n",
    "        load_params['msg_size_bytes'],\n",
    "        load_params['extra_producer_args']['linger.ms'],\n",
    "        (endProduce-start).seconds,\n",
    "        (end-start).seconds,\n",
    "        int(load_params['num_msgs'] * load_params['msg_size_bytes'] / \n",
    "            (end-start).seconds / (1024) ),\n",
    "        mean(latencies)\n",
    "    ]\n",
    "    return result\n",
    "\n",
    "from tabulate import tabulate\n",
    "def print_results_app(results):  \n",
    "    print(tabulate(results, \n",
    "                headers=['Producers', 'Messages', 'Message Size (bytes)', 'linger.ms', 'Produce Time (S)', 'Total Time (s)', 'Throughput (KB/s)', 'Avg Latency (micros)'],\n",
    "                tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linger.ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 120 s Start:2024-02-02 23:25:38.193866+00:00, End:2024-02-02 23:27:38.688618+00:00)\n",
      "Done in 118 s Start:2024-02-02 23:27:38.696706+00:00, End:2024-02-02 23:29:37.587076+00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%6|1706916649.769|FAIL|rdkafka#producer-3| [thrd:sasl_ssl://b7-pkc-lzvrd.us-west4.gcp.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-lzvrd.us-west4.gcp.confluent.cloud:9092/7: Disconnected (after 70907ms in state UP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 121 s Start:2024-02-02 23:29:37.593751+00:00, End:2024-02-02 23:31:39.380020+00:00)\n",
      "Done in 120 s Start:2024-02-02 23:31:39.387398+00:00, End:2024-02-02 23:33:40.355569+00:00)\n",
      "Done in 122 s Start:2024-02-02 23:33:40.359552+00:00, End:2024-02-02 23:35:43.110320+00:00)\n",
      "|   Producers |   Messages |   Message Size (bytes) |   linger.ms |   Produce Time (S) |   Total Time (s) |   Throughput (KB/s) |   Avg Latency (micros) |\n",
      "|-------------+------------+------------------------+-------------+--------------------+------------------+---------------------+------------------------|\n",
      "|           1 |      10000 |                   4096 |           0 |                120 |              120 |                 333 |               0.716422 |\n",
      "|           1 |      10000 |                   4096 |          10 |                118 |              118 |                 338 |               0.718877 |\n",
      "|           1 |      10000 |                   4096 |         100 |                121 |              121 |                 330 |               0.814896 |\n",
      "|           1 |      10000 |                   4096 |        1000 |                120 |              120 |                 333 |               1.38537  |\n",
      "|           1 |      10000 |                   4096 |       10000 |                120 |              122 |                 327 |               7.04322  |\n"
     ]
    }
   ],
   "source": [
    "# Load Generation\n",
    "load_params = {\n",
    "    'num_producers': 1,\n",
    "    'num_msgs': 1000,\n",
    "    'msg_rate_per_s': 100, \n",
    "    'msg_size_bytes': 4*1024,\n",
    "    'extra_producer_args': {\n",
    "        'linger.ms': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "linger_ms_times = [ 0, 10, 100, 1000, 10000 ]\n",
    "\n",
    "results = []\n",
    "for linger_ms in linger_ms_times:\n",
    "    load_params['extra_producer_args']['linger.ms'] = linger_ms\n",
    "    results.append(execute_simulation_app(load_params))\n",
    "\n",
    "print_results_app(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same code but now using python client library metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "def execute_simulation_app_client(load_params):\n",
    "\n",
    "    start, endProduce, end, latencies,stats = publishMessages(load_params)\n",
    "    print(f\"Done in {(end-start).seconds} s Start:{start}, End:{end})\")\n",
    "\n",
    "    # Kafka client library metrics\n",
    "    # Some are aggregated so we only need the last value\n",
    "    # Some are per metric report, so we need to aggregate them ourselves\n",
    "    client_metrics = { \n",
    "        'num_requests_made' : sum( [ s[-1]['tx'] for s in stats.values() ]),\n",
    "        'num_messages_sent' : sum( [ s[-1]['txmsgs'] for s in stats.values() ]),\n",
    "        'num_batch_cnt': sum( t['batchcnt']['cnt'] for s in stats.values() for entry in s for t in entry['topics'].values() ),\n",
    "        'avg_batch_size_bytes': mean( t['batchsize']['avg'] for s in stats.values() for entry in s for t in entry['topics'].values() )\n",
    "    }\n",
    "    \n",
    "    result = [\n",
    "        load_params['num_producers'],\n",
    "        load_params['num_msgs'],\n",
    "        load_params['msg_size_bytes'],\n",
    "        load_params['extra_producer_args']['linger.ms'],\n",
    "        (endProduce-start).seconds,\n",
    "        (end-start).seconds,\n",
    "        int(load_params['num_msgs'] * load_params['msg_size_bytes'] / \n",
    "            (end-start).seconds / (1024) ),\n",
    "        mean(latencies),\n",
    "\n",
    "        client_metrics['num_requests_made'],\n",
    "        client_metrics['num_requests_made'] / (end-start).seconds ,\n",
    "        client_metrics['num_messages_sent'],\n",
    "        client_metrics['num_batch_cnt'],\n",
    "        client_metrics['avg_batch_size_bytes']\n",
    "        \n",
    "    ]\n",
    "    return result\n",
    "\n",
    "from tabulate import tabulate\n",
    "def print_results_app_client(results):  \n",
    "    print(tabulate(results, \n",
    "                headers=['Producers', 'Messages', 'Message Size (bytes)', 'linger.ms', 'Produce Time (S)', 'Total Time (s)', 'Throughput (KB/s)', 'Avg Latency (micros)',\n",
    "                         'Requests', 'Request Rate', 'Messages Sent', 'Batches Sent', 'Avg Batch Size'],\n",
    "                tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 12 s Start:2024-02-02 23:48:58.542527+00:00, End:2024-02-02 23:49:11.000761+00:00)\n",
      "Done in 12 s Start:2024-02-02 23:49:11.005187+00:00, End:2024-02-02 23:49:23.222951+00:00)\n",
      "Done in 12 s Start:2024-02-02 23:49:23.226505+00:00, End:2024-02-02 23:49:35.653324+00:00)\n",
      "Done in 12 s Start:2024-02-02 23:49:35.659026+00:00, End:2024-02-02 23:49:48.224725+00:00)\n",
      "Done in 13 s Start:2024-02-02 23:49:48.229457+00:00, End:2024-02-02 23:50:01.524892+00:00)\n",
      "|   Producers |   Messages |   Message Size (bytes) |   linger.ms |   Produce Time (S) |   Total Time (s) |   Throughput (KB/s) |   Avg Latency (micros) |   Requests |   Request Rate |   Messages Sent |   Batches Sent |   Avg Batch Size |\n",
      "|-------------+------------+------------------------+-------------+--------------------+------------------+---------------------+------------------------+------------+----------------+-----------------+----------------+------------------|\n",
      "|           1 |       1000 |                   4096 |           0 |                 12 |               12 |                 333 |               0.810569 |        916 |       76.3333  |             987 |            897 |          4350.94 |\n",
      "|           1 |       1000 |                   4096 |          10 |                 12 |               12 |                 333 |               0.810962 |        898 |       74.8333  |             987 |            885 |          6363.53 |\n",
      "|           1 |       1000 |                   4096 |         100 |                 12 |               12 |                 333 |               0.877192 |        252 |       21       |            1000 |            239 |         16974.8  |\n",
      "|           1 |       1000 |                   4096 |        1000 |                 12 |               12 |                 333 |               1.30258  |         50 |        4.16667 |            1000 |             36 |         49916.8  |\n",
      "|           1 |       1000 |                   4096 |       10000 |                 12 |               13 |                 307 |               5.96398  |        566 |       43.5385  |            1000 |              6 |         26756.5  |\n"
     ]
    }
   ],
   "source": [
    "# Load Generation\n",
    "load_params = {\n",
    "    'num_producers': 1,\n",
    "    'num_msgs': 1000,\n",
    "    'msg_rate_per_s': 100, \n",
    "    'msg_size_bytes': 4*1024,\n",
    "    'extra_producer_args': {\n",
    "        'linger.ms': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "linger_ms_times = [ 0, 10, 100, 1000, 10000 ]\n",
    "\n",
    "results = []\n",
    "for linger_ms in linger_ms_times:\n",
    "    load_params['extra_producer_args']['linger.ms'] = linger_ms\n",
    "    results.append(execute_simulation_app_client(load_params))\n",
    "\n",
    "print_results_app_client(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the same code but now including a call to the Confluent Cloud Metrics API for cluster metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "def execute_simulation_app_client_cluster(load_params):\n",
    "\n",
    "    start, endProduce, end, latencies,stats = publishMessages(load_params)\n",
    "    print(f\"Done in {(end-start).seconds} s Start:{start}, End:{end})\")\n",
    "\n",
    "    # Kafka client library metrics\n",
    "    # Some are aggregated so we only need the last value\n",
    "    # Some are per metric report, so we need to aggregate them ourselves\n",
    "    client_metrics = { \n",
    "        'num_requests_made' : sum( [ s[-1]['tx'] for s in stats.values() ]),\n",
    "        'num_messages_sent' : sum( [ s[-1]['txmsgs'] for s in stats.values() ]),\n",
    "        'num_batch_cnt': sum( t['batchcnt']['cnt'] for s in stats.values() for entry in s for t in entry['topics'].values() ),\n",
    "        'avg_batch_size_bytes': mean( t['batchsize']['avg'] for s in stats.values() for entry in s for t in entry['topics'].values() )\n",
    "    }\n",
    "    \n",
    "    # Metrics from the cluster metrics API\n",
    "    from_metrics_api = getMetrics(start, end)\n",
    "    cluster_metrics = {\n",
    "        'received_bytes': sum([ v['value'] for v in from_metrics_api['received_bytes']['data'] ]),\n",
    "        'active_connection_count': max([ v['value'] for v in from_metrics_api['active_connection_count']['data'] ]),\n",
    "        'request_count': sum([ v['value'] for v in from_metrics_api['request_count']['data'] ]),\n",
    "        'received_records': sum([ v['value'] for v in from_metrics_api['received_records']['data'] ])\n",
    "    }\n",
    "\n",
    "    result = [\n",
    "        load_params['num_producers'],\n",
    "        load_params['num_msgs'],\n",
    "        load_params['msg_size_bytes'],\n",
    "        load_params['extra_producer_args']['linger.ms'],\n",
    "        (endProduce-start).seconds,\n",
    "        (end-start).seconds,\n",
    "        int(load_params['num_msgs'] * load_params['msg_size_bytes'] / \n",
    "            (end-start).seconds / (1024) ),\n",
    "        mean(latencies),\n",
    "\n",
    "        client_metrics['num_requests_made'],\n",
    "        client_metrics['num_requests_made'] / (end-start).seconds ,\n",
    "        client_metrics['num_messages_sent'],\n",
    "        client_metrics['num_batch_cnt'],\n",
    "        client_metrics['avg_batch_size_bytes'],\n",
    "\n",
    "        cluster_metrics['received_bytes'] / (1024),\n",
    "        cluster_metrics['active_connection_count'],\n",
    "        cluster_metrics['request_count'],\n",
    "        cluster_metrics['received_records']\n",
    "        \n",
    "    ]\n",
    "    return result\n",
    "\n",
    "from tabulate import tabulate\n",
    "def print_results_app_client_cluster(results):  \n",
    "    print(tabulate(results, \n",
    "                headers=['Producers', 'Messages', 'Message Size (bytes)', 'linger.ms', 'Produce Time (S)', 'Total Time (s)', 'Throughput (KB/s)', 'Avg Latency (micros)',\n",
    "                         'Requests', 'Request Rate', 'Messages Sent', 'Batches Sent', 'Avg Batch Size',\n",
    "                         'ingress (MB)','connections', 'Requests', 'Messages'],\n",
    "                tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 121 s Start:2024-02-02 23:54:49.324021+00:00, End:2024-02-02 23:56:50.436093+00:00)\n",
      "Done in 119 s Start:2024-02-02 23:59:53.066745+00:00, End:2024-02-03 00:01:52.793785+00:00)\n",
      "Done in 120 s Start:2024-02-03 00:04:55.320939+00:00, End:2024-02-03 00:06:56.046369+00:00)\n",
      "Done in 121 s Start:2024-02-03 00:09:58.626834+00:00, End:2024-02-03 00:12:00.281963+00:00)\n",
      "Done in 123 s Start:2024-02-03 00:15:03.170053+00:00, End:2024-02-03 00:17:06.987051+00:00)\n",
      "|   Producers |   Messages |   Message Size (bytes) |   linger.ms |   Produce Time (S) |   Total Time (s) |   Throughput (KB/s) |   Avg Latency (micros) |   Requests |   Request Rate |   Messages Sent |   Batches Sent |   Avg Batch Size |   ingress (MB) |   connections |   Requests |   Messages |\n",
      "|-------------+------------+------------------------+-------------+--------------------+------------------+---------------------+------------------------+------------+----------------+-----------------+----------------+------------------+----------------+---------------+------------+------------|\n",
      "|           1 |      10000 |                   4096 |           0 |                121 |              121 |                 330 |               0.717386 |       9929 |       82.0579  |            9997 |           9913 |          4225.34 |        40799.7 |             5 |       9793 |       9975 |\n",
      "|           1 |      10000 |                   4096 |          10 |                119 |              119 |                 336 |               0.724306 |       9816 |       82.4874  |           10000 |           9783 |          4404.07 |        40936.5 |             5 |       9534 |       9912 |\n",
      "|           1 |      10000 |                   4096 |         100 |                120 |              120 |                 333 |               0.811856 |       2556 |       21.3     |            9992 |           2544 |         16524.6  |        40137.9 |             5 |       2550 |      10000 |\n",
      "|           1 |      10000 |                   4096 |        1000 |                121 |              121 |                 330 |               1.39059  |        379 |        3.13223 |           10000 |            357 |         37427.5  |        40772.5 |             5 |        360 |      10000 |\n",
      "|           1 |      10000 |                   4096 |       10000 |                121 |              123 |                 325 |               7.06642  |       5876 |       47.7724  |           10000 |             42 |         32666.6  |        40762   |             5 |         44 |      10000 |\n"
     ]
    }
   ],
   "source": [
    "# Load Generation\n",
    "load_params = {\n",
    "    'num_producers': 1,\n",
    "    'num_msgs': 10000,\n",
    "    'msg_rate_per_s': 100, \n",
    "    'msg_size_bytes': 4*1024,\n",
    "    'extra_producer_args': {\n",
    "        'linger.ms': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "linger_ms_times = [ 0, 10, 100, 1000, 10000 ]\n",
    "\n",
    "results = []\n",
    "for linger_ms in linger_ms_times:\n",
    "    load_params['extra_producer_args']['linger.ms'] = linger_ms\n",
    "    results.append(execute_simulation_app_client_cluster(load_params))\n",
    "    sleep(120) # Cluster metrics are aggregated to the minute and we want to avoid overlap\n",
    "\n",
    "print_results_app_client_cluster(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still bothered by client side requests != cluster request_count. Lets turn debugging on for a small test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 12 s Start:2024-02-03 00:43:01.814426+00:00, End:2024-02-03 00:43:13.986518+00:00)\n",
      "|   Producers |   Messages |   Message Size (bytes) |   linger.ms |   Produce Time (S) |   Total Time (s) |   Throughput (KB/s) |   Avg Latency (micros) |   Requests |   Request Rate |   Messages Sent |   Batches Sent |   Avg Batch Size |   ingress (MB) |   connections |   Requests |   Messages |\n",
      "|-------------+------------+------------------------+-------------+--------------------+------------------+---------------------+------------------------+------------+----------------+-----------------+----------------+------------------+----------------+---------------+------------+------------|\n",
      "|           1 |       1000 |                   4096 |           0 |                 12 |               12 |                 333 |               0.823122 |        902 |        75.1667 |             990 |            862 |          7215.74 |        4126.28 |             1 |        874 |       1000 |\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(logging.FileHandler(filename='publisher.log'))\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Load Generation\n",
    "load_params = {\n",
    "    'num_producers': 1,\n",
    "    'num_msgs': 1000,\n",
    "    'msg_rate_per_s': 100, \n",
    "    'msg_size_bytes': 4*1024,\n",
    "    'extra_producer_args': {\n",
    "        'linger.ms': 0,\n",
    "        'debug': 'all',\n",
    "        'logger': logger\n",
    "    }\n",
    "}\n",
    "\n",
    "results = []\n",
    "results.append(execute_simulation_app_client_cluster(load_params))\n",
    "\n",
    "print_results_app_client_cluster(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out the number of connections by changing the number of producers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 26 s Start:2024-02-03 01:23:20.433946+00:00, End:2024-02-03 01:23:47.012338+00:00)\n",
      "Done in 26 s Start:2024-02-03 01:26:49.505427+00:00, End:2024-02-03 01:27:15.822420+00:00)\n",
      "Done in 11 s Start:2024-02-03 01:30:18.404505+00:00, End:2024-02-03 01:30:29.719345+00:00)\n",
      "Done in 14 s Start:2024-02-03 01:33:32.825905+00:00, End:2024-02-03 01:33:47.565507+00:00)\n",
      "|   Producers |   Messages |   Message Size (bytes) |   linger.ms |   Produce Time (S) |   Total Time (s) |   Throughput (KB/s) |   Avg Latency (micros) |   Requests |   Request Rate |   Messages Sent |   Batches Sent |   Avg Batch Size |   ingress (MB) |   connections |   Requests |   Messages |\n",
      "|-------------+------------+------------------------+-------------+--------------------+------------------+---------------------+------------------------+------------+----------------+-----------------+----------------+------------------+----------------+---------------+------------+------------|\n",
      "|           1 |      20000 |                   2048 |           0 |                  2 |               26 |                1538 |               13.1751  |       7889 |        303.423 |           20000 |             45 |         274239   |        41329.8 |             3 |         47 |      20000 |\n",
      "|           2 |      20000 |                   2048 |           0 |                  2 |               26 |                1538 |               17.5211  |       8335 |        320.577 |           20000 |             46 |         182839   |        41515.2 |             2 |         49 |      20000 |\n",
      "|          10 |      20000 |                   2048 |           0 |                  7 |               11 |                3636 |                3.21178 |       8995 |        817.727 |           20000 |           2916 |          53955.8 |        41683.4 |            12 |       2936 |      20000 |\n",
      "|          20 |      20000 |                   2048 |           0 |                 11 |               14 |                2857 |                3.09953 |      12420 |        887.143 |           20000 |           7498 |          10416.6 |        40906.1 |            17 |       7516 |      19500 |\n"
     ]
    }
   ],
   "source": [
    "# Load Generation\n",
    "load_params = {\n",
    "    'num_producers': 1,\n",
    "    'num_msgs': 20000,\n",
    "    'msg_rate_per_s': 100000, \n",
    "    'msg_size_bytes': 2*1024,\n",
    "    'extra_producer_args': {\n",
    "        'linger.ms': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "num_producers = [ 1, 2, 10, 20 ]\n",
    "\n",
    "results = []\n",
    "for producers in num_producers:\n",
    "    load_params['num_producers'] = producers\n",
    "    results.append(execute_simulation_app_client_cluster(load_params))\n",
    "    sleep(120) # Wait to make sure there is no overlap in cluster metrics\n",
    "\n",
    "\n",
    "print_results_app_client_cluster(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
