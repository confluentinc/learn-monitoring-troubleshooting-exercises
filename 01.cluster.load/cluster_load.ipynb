{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "!python -m pip install matplotlib confluent-kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.cache\n",
    "def read_ccloud_config(config_file='client.properties'):\n",
    "    conf = {}\n",
    "    with open(config_file) as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if len(line) != 0 and line[0] != \"#\":\n",
    "                parameter, value = line.strip().split('=', 1)\n",
    "                conf[parameter] = value.strip()\n",
    "    return conf\n",
    "def read_ccloud_producer_config(config_file='client.properties'):\n",
    "    conf = read_ccloud_config(config_file)\n",
    "    omitted_fields = set(['schema.registry.url', 'basic.auth.credentials.source', 'basic.auth.user.info'])\n",
    "    omitted_prefix = 'confluent'\n",
    "    for fld in list(conf.keys()):\n",
    "        if fld in omitted_fields or fld.startswith(omitted_prefix):\n",
    "            conf.pop(fld, None)\n",
    "    return conf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Generation\n",
    "msg_size_bytes = 1024\n",
    "msg_rate_per_s = 100\n",
    "msg_send_duration_s = 180\n",
    "num_producers = 20 \n",
    "\n",
    "# Cluster Limits from https://docs.confluent.io/cloud/current/clusters/cluster-types.html#types-basic-limits-per-cluster\n",
    "# on 12/1/2023\n",
    "confluent_max = {}\n",
    "confluent_max['active_connection_count'] = 1000\n",
    "confluent_max['received_bytes'] = 250 * 1024 * 1024\n",
    "confluent_max['sent_bytes'] = 750 * 1024 * 1024\n",
    "confluent_max['request_count'] = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Producer Send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delivery_callback(err, msg):\n",
    "    if err:\n",
    "        print('ERROR: Message failed delivery: {}'.format(err))\n",
    "    else:\n",
    "        #print(\"Produced event to topic {topic}: key = {key:12} value = {value:12}\".format(\n",
    "        #    topic=msg.topic(), key=msg.key().decode('utf-8'), value=msg.value().decode('utf-8')))\n",
    "        pass\n",
    "\n",
    "# Produce Messages\n",
    "from confluent_kafka import Producer\n",
    "from time import sleep\n",
    "import json\n",
    "from random import uniform, randint\n",
    "\n",
    "def getMessages(numMessages):\n",
    "    # len is 64\n",
    "    base_msg = \"Upon our honor, we will monitor our data streaming application. \"\n",
    "    for i in range(numMessages):\n",
    "        yield { 'key': f\"mt_key_{randint(1,6)}\", 'value': f\"{base_msg * (msg_size_bytes//len(base_msg))}\" }\n",
    "\n",
    "# Simulating extra connections\n",
    "producers = []\n",
    "for i in range(num_producers):\n",
    "    producers.append(Producer(read_ccloud_producer_config()))\n",
    "    print(\"p\",end=\"\")\n",
    "print()\n",
    "\n",
    "startTime = datetime.now(timezone.utc)\n",
    "msgSentCount = 0\n",
    "for msg in getMessages(msg_rate_per_s * msg_send_duration_s):\n",
    "    ts = datetime.now(timezone.utc)\n",
    "    ts_str = ts.isoformat()\n",
    "    msg['value'] = '{ \"payload\": \"' + msg['value'] + '\", \"ts\": \"' + ts_str + '\" }'\n",
    "    producers[randint(0,num_producers-1)].produce(\"sale_records\", key=msg['key'], value=msg['value'], callback=delivery_callback)\n",
    "    msgSentCount += 1\n",
    "    if not msgSentCount % 1000:\n",
    "        print(msgSentCount)\n",
    "        elapsed_seconds = (ts - startTime).total_seconds()\n",
    "        if msgSentCount >=  elapsed_seconds * msg_rate_per_s:\n",
    "            sleep(msgSentCount/msg_rate_per_s - elapsed_seconds)\n",
    "for producer in producers:\n",
    "    producer.flush()\n",
    "endTime = datetime.now(timezone.utc)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "MetricsQueries = {\n",
    "    'received_bytes': {\n",
    "        'title': 'Ingress',\n",
    "        'yaxis': 'Bytes',\n",
    "        'query': {\n",
    "            'aggregations': [{ 'metric': 'io.confluent.kafka.server/received_bytes'}],\n",
    "            'group_by': ['metric.topic']\n",
    "        }\n",
    "    },\n",
    "    'sent_bytes': {\n",
    "        'title': 'Egress',\n",
    "        'yaxis': 'Bytes',\n",
    "        'query': {\n",
    "            'aggregations': [{ 'metric': 'io.confluent.kafka.server/sent_bytes'}],\n",
    "            'group_by': ['metric.topic']\n",
    "        }\n",
    "    },\n",
    "    'active_connection_count' : {\n",
    "        'title': 'Active Connection Count',\n",
    "        'yaxis': 'Count',\n",
    "        'query': {\n",
    "            \"aggregations\":[{ \"metric\":\"io.confluent.kafka.server/active_connection_count\"}]\n",
    "        }\n",
    "    },\n",
    "    'request_count': {\n",
    "        'title': 'Request Count',\n",
    "        'yaxis': 'Count',\n",
    "        'query': {\n",
    "            'aggregations': [{ 'metric': 'io.confluent.kafka.server/request_count'}]\n",
    "        }\n",
    "    },\n",
    "    'cluster_load': {\n",
    "        'title': 'Cluster Load',\n",
    "        'yaxis': '% Load'\n",
    "    }\n",
    "}\n",
    "def getMetrics(startTime, endTime):\n",
    "    conf = read_ccloud_config()\n",
    "    url = conf['confluent.metrics.endpoint']\n",
    "    headers = {\n",
    "        'Authorization': f\"Basic {conf['confluent.cloud_api_token']}\",\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    common = {\n",
    "        \"filter\":{\"op\":\"OR\",\"filters\":[{\"field\":\"resource.kafka.id\",\"op\":\"EQ\",\"value\":\"lkc-v1jq15\"}]},\n",
    "        \"granularity\":\"PT1M\",\n",
    "        \"limit\":1000\n",
    "    }\n",
    "    interval = {\n",
    "        \"intervals\":[f\"{startTime.isoformat(timespec='seconds')}/{(endTime+timedelta(minutes=1)).isoformat(timespec='seconds')}\"],\n",
    "    }\n",
    "\n",
    "    responses = {}\n",
    "    for qry in MetricsQueries:\n",
    "        if 'query' not in MetricsQueries[qry]:\n",
    "            continue\n",
    "        data = MetricsQueries[qry]['query'] | common | interval\n",
    "\n",
    "        req = urllib.request.Request(url, json.dumps(data).encode('utf-8'), headers)\n",
    "        resp = urllib.request.urlopen(req)\n",
    "        if resp.getcode() == 200:\n",
    "            responses[qry] = json.loads(resp.read())\n",
    "        else:\n",
    "            print(\"Error: {resp.getcode()}\")\n",
    "\n",
    "    return responses\n",
    "# Get Data\n",
    "results = getMetrics(startTime, endTime)\n",
    "cluster_load_data = []\n",
    "# pick any metric to get number of entries since we aggregate per minute\n",
    "for i in range(len(results['active_connection_count']['data'])):\n",
    "     cl_pct = 0\n",
    "     cl_ts = \"\"\n",
    "     for metric in MetricsQueries:\n",
    "        if 'query' not in MetricsQueries[metric] or i >= len(results[metric]['data']):\n",
    "             continue\n",
    "        cl_pct = max(cl_pct, 100.0 * results[metric]['data'][i]['value'] / confluent_max[metric])\n",
    "        cl_ts = max(cl_ts, results[metric]['data'][i]['timestamp'])\n",
    "     cluster_load_data.append({ 'timestamp': cl_ts, 'value': cl_pct })\n",
    "results['cluster_load'] = { 'data': cluster_load_data }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from math import sqrt, ceil\n",
    "\n",
    "\n",
    "def addPlot(axs, row, col, data, title, yAxis):\n",
    "    x_values = [(datetime.fromisoformat(el['timestamp']) - startTime).total_seconds() for el in data]\n",
    "    y_values = [el['value'] for el in data]\n",
    "    axs[row,col].plot(x_values, y_values)\n",
    "\n",
    "    # Add labels and title\n",
    "    axs[row,col].set_xlabel('Time (s)')\n",
    "    axs[row,col].set_ylabel(yAxis.title())\n",
    "    axs[row,col].set_title(title.title())\n",
    "\n",
    "# Display chart but give the metrics a chance to get done\n",
    "# for the ever eager run all in notebook fans\n",
    "timeSinceProduce = (datetime.now(timezone.utc) - endTime).total_seconds()\n",
    "if  timeSinceProduce < 120:\n",
    "    sleep(120-timeSinceProduce)\n",
    "    \n",
    "index = 0\n",
    "total_plots = len(results)\n",
    "ncols = 2\n",
    "nrows = ceil(total_plots/2)\n",
    "fig, axs = plt.subplots(nrows,ncols,sharex=True)\n",
    "for qry in results:\n",
    "    addPlot(axs, index//ncols, index%ncols, results[qry]['data'],  \n",
    "            title=MetricsQueries[qry]['title'], yAxis=MetricsQueries[qry]['yaxis'])\n",
    "    index += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
