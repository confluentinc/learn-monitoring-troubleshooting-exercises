{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunks of code we will need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install confluent-kafka tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of utility code to keep the credentials out of the github repo. There is an easy startup guide for using Confluent Cloud over at https://developer.confluent.io/get-started/python/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# Read the config file; cache it\n",
    "@functools.cache\n",
    "def read_ccloud_config(config_file='client.properties'):\n",
    "    conf = {}\n",
    "    with open(config_file) as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if len(line) != 0 and line[0] != \"#\":\n",
    "                parameter, value = line.strip().split('=', 1)\n",
    "                conf[parameter] = value.strip()\n",
    "    return conf\n",
    "\n",
    "# Skip non-Kafka client properties\n",
    "def read_ccloud_producer_config(config_file='client.properties'):\n",
    "    conf = read_ccloud_config(config_file)\n",
    "    omitted_fields = set(['schema.registry.url', 'basic.auth.credentials.source', 'basic.auth.user.info'])\n",
    "    omitted_prefix = 'confluent'\n",
    "    for fld in list(conf.keys()):\n",
    "        if fld in omitted_fields or fld.startswith(omitted_prefix):\n",
    "            conf.pop(fld, None)\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic Kafka Producer with a simple approach to data rate, payload size, and keys\n",
    "Some documentation:\n",
    "- The python kafka client library is described at https://docs.confluent.io/kafka-clients/python/current/overview.html\n",
    "- The metrics delivered via the callback are documented at librdkafka https://github.com/confluentinc/librdkafka/blob/master/STATISTICS.md\n",
    "- Configuration parameters for the Publisher are also as librdkafka https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md\n",
    "- Callbacks are at the python client documentation https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html#kafka-client-configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timezone\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import json\n",
    "\n",
    "\n",
    "def get_stats_cb(results):\n",
    "    # Accumulating, sort of, the statistics so we can aggregate at the end\n",
    "    def stats_cb(s):\n",
    "        j = json.loads(s)\n",
    "        # Overwriting each time since we just need the last one\n",
    "        results[j['name']].append(j)\n",
    "    return stats_cb\n",
    "        \n",
    "def get_delivery_callback(latencies):\n",
    "    def delivery_callback(err, msg):\n",
    "        if err:\n",
    "            print('ERROR: Message failed delivery: {}'.format(err))\n",
    "        else:\n",
    "            latencies.append(msg.latency())\n",
    "    return delivery_callback\n",
    "\n",
    "def getMessages(numMessages, msgSize):\n",
    "    num_partitions = 6 # Our topic is configured as such\n",
    "    # len is 64 for the below string\n",
    "    base_msg = \"Upon our honor, we will monitor our data streaming application. \"\n",
    "    for i in range(numMessages):\n",
    "        yield { 'key': f\"mt_key_{i % num_partitions}\", 'value': f\"{base_msg * (msgSize//len(base_msg))}\" }\n",
    "\n",
    "def publishMessages(load_params):\n",
    "    startTime = datetime.now(timezone.utc)\n",
    "\n",
    "    # Simulating extra connections\n",
    "    conf = read_ccloud_producer_config()\n",
    "    statistics_interval_ms = 250\n",
    "    stats = defaultdict(list)\n",
    "    conf['stats_cb'] = get_stats_cb(stats)\n",
    "    conf['statistics.interval.ms'] = statistics_interval_ms\n",
    "    if 'extra_producer_args' in load_params.keys():\n",
    "        conf.update(load_params['extra_producer_args'])\n",
    "\n",
    "    producers = [ Producer(conf) for i in range(load_params['num_producers']) ]\n",
    "    \n",
    "    msgSentCount = 0\n",
    "    numMessages = load_params['num_msgs']\n",
    "    msgSize = load_params['msg_size_bytes']\n",
    "    msgRateSleepTimeSecs = 1 / load_params['msg_rate_per_s'] \n",
    "\n",
    "    latencies = []\n",
    "    delivery_callback = get_delivery_callback(latencies)\n",
    "\n",
    "    for msg in getMessages(numMessages, msgSize):\n",
    "        ts = datetime.now(timezone.utc)\n",
    "        ts_str = ts.isoformat()\n",
    "        msg['value'] = '{ \"payload\": \"' + msg['value'] + '\", \"ts\": \"' + ts_str + '\" }'\n",
    "        producer_index = msgSentCount % load_params['num_producers']\n",
    "        producers[producer_index].produce(\"sale_records\", key=msg['key'], value=msg['value'],\n",
    "                                                       callback=delivery_callback)\n",
    "        if not msgSentCount % 100:\n",
    "            for producer in producers:\n",
    "                producer.poll()\n",
    "        msgSentCount += 1\n",
    "        sleep(msgRateSleepTimeSecs)           \n",
    "\n",
    "    produceEndTime = datetime.now(timezone.utc)\n",
    "\n",
    "    for producer in producers:\n",
    "        producer.flush()\n",
    "    \n",
    "    endTime = datetime.now(timezone.utc)\n",
    "\n",
    "    return startTime, produceEndTime, endTime, latencies, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calls to the Confluent Metrics API to get the metrics we care about. \n",
    "- Documentation for it is at https://docs.confluent.io/cloud/current/monitoring/metrics-api.html. \n",
    "- A complete list of available cluster metrics is at https://api.telemetry.confluent.cloud/docs/descriptors/datasets/cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "MetricsQueries = {\n",
    "    'received_bytes': {\n",
    "        'query': {\n",
    "            'aggregations': [{ 'metric': 'io.confluent.kafka.server/received_bytes'}],\n",
    "            'group_by': ['metric.topic']\n",
    "        }\n",
    "    },\n",
    "    'active_connection_count' : {\n",
    "        'query': { \"aggregations\":[{ \"metric\":\"io.confluent.kafka.server/active_connection_count\"}] }\n",
    "    },\n",
    "    'request_count': {\n",
    "        'query': { 'aggregations': [{ 'metric': 'io.confluent.kafka.server/request_count'}] }\n",
    "    },\n",
    "    'received_records': {\n",
    "        'query': { 'aggregations': [{ 'metric': 'io.confluent.kafka.server/received_records'}]}\n",
    "    }\n",
    "}\n",
    "def getMetrics(startTime, endTime):\n",
    "    # The Metrics API aggregates by the minute, and throw in clock skew\n",
    "    sleep(60)\n",
    "    startTime -= timedelta(seconds=60)\n",
    "    endTime += timedelta(seconds=60)\n",
    "\n",
    "    conf = read_ccloud_config()\n",
    "    url = conf['confluent.metrics.endpoint']\n",
    "    headers = {\n",
    "        'Authorization': f\"Basic {conf['confluent.cloud_api_token']}\",\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    common = {\n",
    "        \"filter\":{\"op\":\"OR\",\"filters\":[{\"field\":\"resource.kafka.id\",\"op\":\"EQ\",\"value\":\"lkc-v1jq15\"}]},\n",
    "        \"granularity\":\"PT1M\",\n",
    "        \"limit\":1000\n",
    "    }\n",
    "    interval = {\n",
    "        \"intervals\":[f\"{startTime.isoformat(timespec='seconds')}/{endTime.isoformat(timespec='seconds')}\"],\n",
    "    }\n",
    "\n",
    "    responses = {}\n",
    "    for qry in MetricsQueries:\n",
    "        data = MetricsQueries[qry]['query'] | common | interval\n",
    "\n",
    "        req = urllib.request.Request(url, json.dumps(data).encode('utf-8'), headers)\n",
    "        resp = urllib.request.urlopen(req)\n",
    "        if resp.getcode() == 200:\n",
    "            responses[qry] = json.loads(resp.read())\n",
    "        else:\n",
    "            print(f\"Error: {resp.getcode()}, Request was {json.dumps(data)}\")\n",
    "\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linger.ms with application metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main code to execute a sample load. Given parameters, it sends out all the messages, and returns a few metrics that it generates. It also includes a simple tabular print utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "def execute_simulation_app(load_params):\n",
    "\n",
    "    start, endProduce, end, latencies,stats = publishMessages(load_params)\n",
    "    print(f\"Done in {(end-start).seconds} s Start:{start}, End:{end})\")\n",
    "\n",
    "    result = [\n",
    "        load_params['num_producers'],\n",
    "        load_params['num_msgs'],\n",
    "        load_params['msg_size_bytes'],\n",
    "        load_params['extra_producer_args']['linger.ms'],\n",
    "        (endProduce-start).seconds,\n",
    "        (end-start).seconds,\n",
    "        int(load_params['num_msgs'] * load_params['msg_size_bytes'] / \n",
    "            (end-start).seconds / (1024) ),\n",
    "        mean(latencies)\n",
    "    ]\n",
    "    return result\n",
    "\n",
    "from tabulate import tabulate\n",
    "def print_results_app(results):  \n",
    "    print(tabulate(results, \n",
    "                headers=['Producers', 'Messages', 'Message Size (bytes)', 'linger.ms', 'Produce Time (S)', 'Total Time (s)', 'Throughput (KB/s)', 'Avg Latency (micros)'],\n",
    "                tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Generation\n",
    "load_params = {\n",
    "    'num_producers': 1,\n",
    "    'num_msgs': 4000,\n",
    "    'msg_rate_per_s': 200, \n",
    "    'msg_size_bytes': 4*1024,\n",
    "    'extra_producer_args': {\n",
    "        'linger.ms': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "linger_ms_times = [ 0, 10, 100, 1000, 10000 ]\n",
    "\n",
    "results = []\n",
    "for linger_ms in linger_ms_times:\n",
    "    load_params['extra_producer_args']['linger.ms'] = linger_ms\n",
    "    results.append(execute_simulation_app(load_params))\n",
    "\n",
    "print_results_app(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linger.ms with application, and kafka client metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same code but now includes getting the python client library metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "def execute_simulation_app_client(load_params):\n",
    "\n",
    "    start, endProduce, end, latencies,stats = publishMessages(load_params)\n",
    "    print(f\"Done in {(end-start).seconds} s Start:{start}, End:{end})\")\n",
    "\n",
    "    # Kafka client library metrics\n",
    "    # Some are aggregated so we only need the last value\n",
    "    # Some are per metric report, so we need to aggregate them ourselves\n",
    "    client_metrics = { \n",
    "        'num_requests_made' : sum( [ s[-1]['tx'] for s in stats.values() ]),\n",
    "        'num_messages_sent' : sum( [ s[-1]['txmsgs'] for s in stats.values() ]),\n",
    "        'num_batch_cnt': sum( t['batchcnt']['cnt'] for s in stats.values() for entry in s for t in entry['topics'].values() ),\n",
    "        'avg_batch_size_bytes': mean( t['batchsize']['avg'] for s in stats.values() for entry in s for t in entry['topics'].values() )\n",
    "    }\n",
    "    \n",
    "    result = [\n",
    "        load_params['num_producers'],\n",
    "        load_params['num_msgs'],\n",
    "        load_params['msg_size_bytes'],\n",
    "        load_params['extra_producer_args']['linger.ms'],\n",
    "        (endProduce-start).seconds,\n",
    "        (end-start).seconds,\n",
    "        int(load_params['num_msgs'] * load_params['msg_size_bytes'] / \n",
    "            (end-start).seconds / (1024) ),\n",
    "        mean(latencies),\n",
    "\n",
    "        client_metrics['num_requests_made'],\n",
    "        client_metrics['num_requests_made'] / (end-start).seconds ,\n",
    "        client_metrics['num_messages_sent'],\n",
    "        client_metrics['num_batch_cnt'],\n",
    "        client_metrics['avg_batch_size_bytes']\n",
    "        \n",
    "    ]\n",
    "    return result\n",
    "\n",
    "from tabulate import tabulate\n",
    "def print_results_app_client(results):  \n",
    "    print(tabulate(results, \n",
    "                headers=['Producers', 'Messages', 'Message Size (bytes)', 'linger.ms', 'Produce Time (S)', 'Total Time (s)', 'Throughput (KB/s)', 'Avg Latency (micros)',\n",
    "                         'Requests', 'Request Rate', 'Messages Sent', 'Batches Sent', 'Avg Batch Size'],\n",
    "                tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Generation\n",
    "load_params = {\n",
    "    'num_producers': 1,\n",
    "    'num_msgs': 4000,\n",
    "    'msg_rate_per_s': 200, \n",
    "    'msg_size_bytes': 4*1024,\n",
    "    'extra_producer_args': {\n",
    "        'linger.ms': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "linger_ms_times = [ 0, 10, 100, 1000, 10000 ]\n",
    "\n",
    "results = []\n",
    "for linger_ms in linger_ms_times:\n",
    "    load_params['extra_producer_args']['linger.ms'] = linger_ms\n",
    "    results.append(execute_simulation_app_client(load_params))\n",
    "\n",
    "print_results_app_client(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linger.ms with application, kafka client and cluster metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the same code but now including a call to the Confluent Cloud Metrics API for cluster metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "def execute_simulation_app_client_cluster(load_params):\n",
    "\n",
    "    start, endProduce, end, latencies,stats = publishMessages(load_params)\n",
    "    print(f\"Done in {(end-start).seconds} s Start:{start}, End:{end})\")\n",
    "\n",
    "    # Kafka client library metrics\n",
    "    # Some are aggregated so we only need the last value\n",
    "    # Some are per metric report, so we need to aggregate them ourselves\n",
    "    client_metrics = { \n",
    "        'num_requests_made' : sum( [ s[-1]['tx'] for s in stats.values() ]),\n",
    "        'num_messages_sent' : sum( [ s[-1]['txmsgs'] for s in stats.values() ]),\n",
    "        'num_batch_cnt': sum( t['batchcnt']['cnt'] for s in stats.values() for entry in s for t in entry['topics'].values() ),\n",
    "        'avg_batch_size_bytes': mean( t['batchsize']['avg'] for s in stats.values() for entry in s for t in entry['topics'].values() )\n",
    "    }\n",
    "    \n",
    "    # Metrics from the cluster metrics API\n",
    "    from_metrics_api = getMetrics(start, end)\n",
    "    cluster_metrics = {\n",
    "        'received_bytes': sum([ v['value'] for v in from_metrics_api['received_bytes']['data'] ]),\n",
    "        'active_connection_count': max([ v['value'] for v in from_metrics_api['active_connection_count']['data'] ]),\n",
    "        'request_count': sum([ v['value'] for v in from_metrics_api['request_count']['data'] ]),\n",
    "        'received_records': sum([ v['value'] for v in from_metrics_api['received_records']['data'] ])\n",
    "    }\n",
    "\n",
    "    result = [\n",
    "        load_params['num_producers'],\n",
    "        load_params['num_msgs'],\n",
    "        load_params['msg_size_bytes'],\n",
    "        load_params['extra_producer_args']['linger.ms'],\n",
    "        (endProduce-start).seconds,\n",
    "        (end-start).seconds,\n",
    "        int(load_params['num_msgs'] * load_params['msg_size_bytes'] / \n",
    "            (end-start).seconds / (1024) ),\n",
    "        mean(latencies),\n",
    "\n",
    "        client_metrics['num_requests_made'],\n",
    "        client_metrics['num_requests_made'] / (end-start).seconds ,\n",
    "        client_metrics['num_messages_sent'],\n",
    "        client_metrics['num_batch_cnt'],\n",
    "        client_metrics['avg_batch_size_bytes'],\n",
    "\n",
    "        cluster_metrics['received_bytes'] / (1024),\n",
    "        cluster_metrics['active_connection_count'],\n",
    "        cluster_metrics['request_count'],\n",
    "        cluster_metrics['received_records']\n",
    "        \n",
    "    ]\n",
    "    return result\n",
    "\n",
    "from tabulate import tabulate\n",
    "def print_results_app_client_cluster(results):  \n",
    "    print(tabulate(results, \n",
    "                headers=['Producers', 'Messages', 'Message Size (bytes)', 'linger.ms', 'Produce Time (S)', 'Total Time (s)', 'Throughput (KB/s)', 'Avg Latency (micros)',\n",
    "                         'Requests', 'Request Rate', 'Messages Sent', 'Batches Sent', 'Avg Batch Size',\n",
    "                         'ingress (KB)','connections', 'Requests', 'Messages'],\n",
    "                tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Generation\n",
    "load_params = {\n",
    "    'num_producers': 1,\n",
    "    'num_msgs': 4000,\n",
    "    'msg_rate_per_s': 200, \n",
    "    'msg_size_bytes': 4*1024,\n",
    "    'extra_producer_args': {\n",
    "        'linger.ms': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "linger_ms_times = [ 0, 10, 100, 1000, 10000 ]\n",
    "\n",
    "results = []\n",
    "for linger_ms in linger_ms_times:\n",
    "    load_params['extra_producer_args']['linger.ms'] = linger_ms\n",
    "    results.append(execute_simulation_app_client_cluster(load_params))\n",
    "    sleep(120) # Cluster metrics are aggregated to the minute and we want to avoid overlap\n",
    "\n",
    "print_results_app_client_cluster(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still bothered by client side requests != cluster request_count. Lets turn debugging on for a small test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(logging.FileHandler(filename='publisher.log'))\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Load Generation\n",
    "load_params = {\n",
    "    'num_producers': 1,\n",
    "    'num_msgs': 1000,\n",
    "    'msg_rate_per_s': 100, \n",
    "    'msg_size_bytes': 4*1024,\n",
    "    'extra_producer_args': {\n",
    "        'linger.ms': 0,\n",
    "        'debug': 'all',\n",
    "        'logger': logger\n",
    "    }\n",
    "}\n",
    "\n",
    "results = []\n",
    "results.append(execute_simulation_app_client_cluster(load_params))\n",
    "\n",
    "print_results_app_client_cluster(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out the number of connections by changing the number of producers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Generation\n",
    "load_params = {\n",
    "    'num_producers': 1,\n",
    "    'num_msgs': 50000,\n",
    "    'msg_rate_per_s': 100000, \n",
    "    'msg_size_bytes': 4*1024,\n",
    "    'extra_producer_args': {\n",
    "        'linger.ms': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "num_producers = [ 1, 2, 10, 20, 40 ]\n",
    "\n",
    "results = []\n",
    "for producers in num_producers:\n",
    "    load_params['num_producers'] = producers\n",
    "    results.append(execute_simulation_app_client_cluster(load_params))\n",
    "    sleep(120) # Wait to make sure there is no overlap in cluster metrics\n",
    "\n",
    "\n",
    "print_results_app_client_cluster(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
