{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunks of code we will need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utility code for credentials and configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install confluent-kafka tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of utility code to keep the credentials out of the github repo. There is an easy startup guide for using Confluent Cloud over at https://developer.confluent.io/get-started/python/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the config file\n",
    "def read_ccloud_config(config_file='client.properties', producer_only=True):\n",
    "    omitted_fields = set(['schema.registry.url', 'basic.auth.credentials.source', 'basic.auth.user.info'])\n",
    "    omitted_prefix = 'confluent'\n",
    "    conf = {}\n",
    "    with open(config_file) as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if len(line) != 0 and line[0] != \"#\":\n",
    "                parameter, value = line.strip().split('=', 1)\n",
    "                if producer_only:\n",
    "                    if parameter in omitted_fields or parameter.startswith(omitted_prefix):\n",
    "                        continue   \n",
    "                conf[parameter] = value.strip()\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The message producer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic Kafka Producer with a simple approach to data rate, payload size, and keys\n",
    "Some documentation:\n",
    "- The python kafka client library is described at https://docs.confluent.io/kafka-clients/python/current/overview.html\n",
    "- The metrics delivered via the callback are documented at librdkafka https://github.com/confluentinc/librdkafka/blob/master/STATISTICS.md\n",
    "- Configuration parameters for the Publisher are also as librdkafka https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md\n",
    "- Callbacks are at the python client documentation https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html#kafka-client-configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timezone\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import json\n",
    "\n",
    "\n",
    "def get_stats_cb(results):\n",
    "    # Accumulating, sort of, the statistics so we can aggregate at the end\n",
    "    def stats_cb(s):\n",
    "        j = json.loads(s)\n",
    "        # Overwriting each time since we just need the last one\n",
    "        results[j['name']].append(j)\n",
    "    return stats_cb\n",
    "        \n",
    "def get_delivery_callback(latencies):\n",
    "    def delivery_callback(err, msg):\n",
    "        if err:\n",
    "            print('ERROR: Message failed delivery: {}'.format(err))\n",
    "        else:\n",
    "            latencies.append(msg.latency())\n",
    "    return delivery_callback\n",
    "\n",
    "def getMessages(numMessages, msgSize):\n",
    "    num_partitions = 6 # Our topic is configured as such\n",
    "    # len is 64 for the below string\n",
    "    base_msg = \"Upon our honor, we will monitor our data streaming application. \"\n",
    "    for i in range(numMessages):\n",
    "        yield { 'key': f\"mt_key_{i % num_partitions}\", 'value': f\"{base_msg * (msgSize//len(base_msg))}\" }\n",
    "\n",
    "def publishMessages(load_params):\n",
    "    startTime = datetime.now(timezone.utc)\n",
    "\n",
    "    # Simulating extra connections\n",
    "    conf = read_ccloud_config()\n",
    "    statistics_interval_ms = 250\n",
    "    stats = defaultdict(list)\n",
    "    conf['stats_cb'] = get_stats_cb(stats)\n",
    "    conf['statistics.interval.ms'] = statistics_interval_ms\n",
    "    if 'extra_producer_args' in load_params.keys():\n",
    "        conf.update(load_params['extra_producer_args'])\n",
    "\n",
    "    producers = [ Producer(conf) for i in range(load_params['num_producers']) ]\n",
    "    \n",
    "    msgSentCount = 0\n",
    "    numMessages = load_params['num_msgs']\n",
    "    msgSize = load_params['msg_size_bytes']\n",
    "    msgRateSleepTimeSecs = 1 / load_params['msg_rate_per_s'] \n",
    "\n",
    "    latencies = []\n",
    "    delivery_callback = get_delivery_callback(latencies)\n",
    "\n",
    "    for msg in getMessages(numMessages, msgSize):\n",
    "        ts = datetime.now(timezone.utc)\n",
    "        ts_str = ts.isoformat()\n",
    "        msg['value'] = '{ \"payload\": \"' + msg['value'] + '\", \"ts\": \"' + ts_str + '\" }'\n",
    "        producer_index = msgSentCount % load_params['num_producers']\n",
    "        producers[producer_index].produce(\"sale_records\", key=msg['key'], value=msg['value'],\n",
    "                                                       on_delivery=delivery_callback)\n",
    "        if not msgSentCount % 100:\n",
    "            for producer in producers:\n",
    "                producer.poll()\n",
    "        msgSentCount += 1\n",
    "        sleep(msgRateSleepTimeSecs)           \n",
    "\n",
    "    produceEndTime = datetime.now(timezone.utc)\n",
    "\n",
    "    for producer in producers:\n",
    "        producer.flush()\n",
    "    \n",
    "    endTime = datetime.now(timezone.utc)\n",
    "\n",
    "    return startTime, produceEndTime, endTime, latencies, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting cluster metrics using the Metrics API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calls to the Confluent Metrics API to get the metrics we care about. \n",
    "- Documentation for it is at https://docs.confluent.io/cloud/current/monitoring/metrics-api.html. \n",
    "- A complete list of available cluster metrics is at https://api.telemetry.confluent.cloud/docs/descriptors/datasets/cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "MetricsQueries = {\n",
    "    'active_connection_count' : {\n",
    "        'query': { \"aggregations\":[{ \"metric\":\"io.confluent.kafka.server/active_connection_count\"}] }\n",
    "    },\n",
    "    'request_count': {\n",
    "        'query': { 'aggregations': [{ 'metric': 'io.confluent.kafka.server/request_count'}] }\n",
    "    },\n",
    "    'received_records': {\n",
    "        'query': { 'aggregations': [{ 'metric': 'io.confluent.kafka.server/received_records'}]}\n",
    "    }\n",
    "}\n",
    "def getMetrics(startTime, endTime):\n",
    "    # The Metrics API aggregates by the minute, and throw in clock skew\n",
    "    sleep(60)\n",
    "    startTime -= timedelta(seconds=60)\n",
    "    endTime += timedelta(seconds=60)\n",
    "\n",
    "    conf = read_ccloud_config(producer_only=False)\n",
    "    url = conf['confluent.metrics.endpoint']\n",
    "    headers = {\n",
    "        'Authorization': f\"Basic {conf['confluent.cloud_api_token']}\",\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    common = {\n",
    "        \"filter\":{\"op\":\"OR\",\"filters\":[{\"field\":\"resource.kafka.id\",\"op\":\"EQ\",\"value\":\"lkc-v1jq15\"}]},\n",
    "        \"granularity\":\"PT1M\",\n",
    "        \"limit\":1000\n",
    "    }\n",
    "    interval = {\n",
    "        \"intervals\":[f\"{startTime.isoformat(timespec='seconds')}/{endTime.isoformat(timespec='seconds')}\"],\n",
    "    }\n",
    "\n",
    "    responses = {}\n",
    "    for qry in MetricsQueries:\n",
    "        data = MetricsQueries[qry]['query'] | common | interval\n",
    "\n",
    "        req = urllib.request.Request(url, json.dumps(data).encode('utf-8'), headers)\n",
    "        resp = urllib.request.urlopen(req)\n",
    "        if resp.getcode() == 200:\n",
    "            responses[qry] = json.loads(resp.read())\n",
    "        else:\n",
    "            print(f\"Error: {resp.getcode()}, Request was {json.dumps(data)}\")\n",
    "\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main code to execute a sample load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "def execute_test(load_params):\n",
    "\n",
    "    start, endProduce, end, latencies,stats = publishMessages(load_params)\n",
    "    print(f\"Done in {(end-start).seconds} s Start:{start}, End:{end})\")\n",
    "\n",
    "    result = [\n",
    "        load_params['extra_producer_args']['linger.ms'],\n",
    "        mean(latencies)\n",
    "    ]\n",
    "    if load_params.get('include_client_metrics', False):\n",
    "        # Kafka client library metrics\n",
    "        # Some are aggregated so we only need the last value\n",
    "        # Some are per metric report, so we need to aggregate them ourselves\n",
    "        client_metrics = { \n",
    "            'num_requests_made' : sum( [ s[-1]['tx'] for s in stats.values() ]),\n",
    "            'num_messages_sent' : sum( [ s[-1]['txmsgs'] for s in stats.values() ]),\n",
    "            'num_batch_cnt': sum( t['batchcnt']['cnt'] for s in stats.values() for entry in s for t in entry['topics'].values() ),\n",
    "            'avg_batch_size_bytes': mean( t['batchsize']['avg'] for s in stats.values() for entry in s for t in entry['topics'].values() )\n",
    "        }\n",
    "        result.extend([\n",
    "            client_metrics['num_requests_made'],\n",
    "            client_metrics['num_messages_sent'],\n",
    "            client_metrics['num_batch_cnt'],\n",
    "            client_metrics['avg_batch_size_bytes']\n",
    "        ])\n",
    "\n",
    "    if load_params.get('include_cluster_metrics', False):\n",
    "        # Metrics from the cluster metrics API\n",
    "        from_metrics_api = getMetrics(start, end)\n",
    "        cluster_metrics = {\n",
    "            'active_connection_count': max([ v['value'] for v in from_metrics_api['active_connection_count']['data'] ]),\n",
    "            'request_count': sum([ v['value'] for v in from_metrics_api['request_count']['data'] ]),\n",
    "            'received_records': sum([ v['value'] for v in from_metrics_api['received_records']['data'] ])\n",
    "        }\n",
    "        result.extend([\n",
    "            cluster_metrics['active_connection_count'],\n",
    "            cluster_metrics['request_count'],\n",
    "            cluster_metrics['received_records']\n",
    "        ])\n",
    "    \n",
    "    return result\n",
    "\n",
    "from tabulate import tabulate\n",
    "def print_results(load_params, results):  \n",
    "    headers = ['linger.ms', 'Avg Latency (micros)']\n",
    "    if load_params.get('include_client_metrics', False):\n",
    "        headers.extend(['Requests', 'Messages Sent', 'Batches Sent', 'Avg Batch Size(bytes)'])\n",
    "    if load_params.get('include_cluster_metrics', False):\n",
    "        headers.extend(['connections', 'Requests', 'Messages'])\n",
    "    print(tabulate(results, headers=headers, tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delivery callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = Producer(read_ccloud_config())\n",
    "producer.produce(topic='sale_records', \n",
    "                 key='msg_key', value='This Data is in Motion',\n",
    "                 on_delivery=lambda err,msg: print(f\"Latency (in microseconds): {msg.latency()}\"))\n",
    "msgs_in_buffer = producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stats callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_callback(json_string):\n",
    "    stats = json.loads(json_string)\n",
    "    print(f\"Messages sent: {stats['txmsgs']}, Avg Messages in Batch: {stats['topics']['sale_records']['batchcnt']['avg']}\")\n",
    "\n",
    "conf = read_ccloud_config()\n",
    "conf['stats_cb'] = stats_callback\n",
    "conf['statistics.interval.ms'] = 100\n",
    "\n",
    "producer = Producer(conf)\n",
    "\n",
    "for msg_cont in range(5):\n",
    "    producer.produce(topic='sale_records', \n",
    "                     key='msg_key', value='This Data is in Motion',\n",
    "                     on_delivery=lambda err,msg: print(f\"Latency (in microseconds): {msg.latency()}\"))\n",
    "    producer.poll()\n",
    "\n",
    "msgs_in_buffer = producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = read_ccloud_config(producer_only=False)\n",
    "url = conf['confluent.metrics.endpoint']\n",
    "startTime = datetime.now(timezone.utc) - timedelta(hours=1)\n",
    "endTime = datetime.now(timezone.utc)\n",
    "headers = {\n",
    "    'Authorization': f\"Basic {conf['confluent.cloud_api_token']}\",\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "qry = {\n",
    "    'aggregations': [{ 'metric': 'io.confluent.kafka.server/received_records'}],\n",
    "    'filter':{'op':'OR','filters':[{'field':'resource.kafka.id','op':'EQ','value':'lkc-v1jq15'}]},\n",
    "    'granularity':'PT1M',\n",
    "    'limit':1000,\n",
    "    'intervals':[f\"{startTime.isoformat(timespec='seconds')}/{endTime.isoformat(timespec='seconds')}\"]\n",
    "}\n",
    "req = urllib.request.Request(url, json.dumps(qry).encode('utf-8'), headers)\n",
    "resp = urllib.request.urlopen(req)\n",
    "print(json.dumps(json.loads(resp.read()), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linger.ms with application, kafka client and cluster metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Generation\n",
    "load_params = {\n",
    "    'num_producers': 1,\n",
    "    'num_msgs': 10000,\n",
    "    'msg_rate_per_s': 200, \n",
    "    'msg_size_bytes': 1*1024,\n",
    "    'extra_producer_args': {\n",
    "        'linger.ms': 0\n",
    "    },\n",
    "    'include_client_metrics': True,\n",
    "    'include_cluster_metrics': True\n",
    "}\n",
    "\n",
    "linger_ms_times = [ 0, 1000 ]\n",
    "\n",
    "results = []\n",
    "for linger_ms in linger_ms_times:\n",
    "    load_params['extra_producer_args']['linger.ms'] = linger_ms\n",
    "    results.append(execute_test(load_params))\n",
    "    sleep(120) # Cluster metrics are aggregated to the minute and we want to avoid overlap\n",
    "\n",
    "print_results(load_params, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still bothered by client side requests != cluster request_count. Lets turn debugging on for a small test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='producer.log', filemode='w',\n",
    "                    level=logging.DEBUG,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "# Load Generation\n",
    "load_params = {\n",
    "    'num_producers': 1,\n",
    "    'num_msgs': 100000,\n",
    "    'msg_rate_per_s': 400, \n",
    "    'msg_size_bytes': 1*1024,\n",
    "    'extra_producer_args': {\n",
    "        'linger.ms': 0,\n",
    "        'debug': 'all',\n",
    "        'logger': logging.getLogger()\n",
    "    },\n",
    "    'include_client_metrics': True,\n",
    "    'include_cluster_metrics': True\n",
    "}\n",
    "\n",
    "results.append(execute_test(load_params))\n",
    "\n",
    "\n",
    "print_results(load_params, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
